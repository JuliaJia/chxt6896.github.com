---
layout: post
title: Hadoop1.0.4 集群配置
category: hadoop
---

h2. 平台环境

* CentOS 5.X
* ssh 
* rsyn
* JDK7  
* 三台机器
** 192.168.1.111 node111
** 192.168.1.112 node112
** 192.168.1.113 node113

h2. Step1 Install Hadoop

h3. Hadoop 使用最新稳定版 1.0.4

{% highlight bash linenos %}
# 官网 http://hadoop.apache.org/ 下载 Hadoop 项目
$ wget http://www.apache.org/dist/hadoop/core/hadoop-1.0.4/hadoop-1.0.4.tar.gz
# 自定义解压在 /home/hadoop/cluster/ 下
$ tar xzvf hadoop-0.20.2.tar.gz
$ cd hadoop-1.0.4
{% endhighlight %}

在 ~/.bashrc 中加入环境变量：

{% highlight bash linenos %}
export JAVA_HOME="/home/hadoop/jdk1.7.0_07"
export HADOOP_HOME="/home/hadoop/cluster/hadoop-1.0.4"
export PATH=$PATH:$JAVA_HOME/bin:$HADOOP_HOME/bin
{% endhighlight %} 

在 /etc/hosts 中加入：

{% highlight bash linenos %}
192.168.1.111 node111
192.168.1.112 node112
192.168.1.113 node113
{% endhighlight %}

h3. 以上操作需要在每台机器上执行

h2. Step2 配置集群

根据 Hadoop 文档的描述 "The Hadoop daemons are NameNode/DataNode and JobTracker/TaskTracker." 可以看出 Hadoop 核心守护程序就是由 NameNode/DataNode 和 JobTracker/TaskTracker 这几个角色构成。
Hadoop的DFS 需要确立 NameNode 与 DataNode 角色，一般 NameNode 会部署到一台单独的服务器上而不与 DataNode 共同同一机器。另外 Map/Reduce 服务也需要确立 JobTracker 和 TaskTracker 的角色，一般 JobTracker 与 NameNode 共用一台机器作为 master，而 TaskTracker 与 DataNode 同属于 slave。

这里使用 node111 作为 NameNode 与 JobTracker ，其它两台机器作为 DataNode 和 TaskTracker

h2. Step2-1 环境配置

在 $HADOOP_HOME/conf/hadoop-env.sh 中定义了 Hadoop 启动时需要的环境变量设置，需要配置 JAVA_HOME（JDK的路径）变量:

{% highlight bash linenos %}
# The java implementation to use.  Required.
export JAVA_HOME=/home/hadoop/jdk1.7.0_07
{% endhighlight %} 

h2. Step2-2 Hadoop 核心配置

Hadoop 包括一组默认配置文件（$HADOOP_HOME/src 目录下的 core/core-default.xml, hdfs/hdfs-default.xml 和 mapred/mapred-default.xml），先好好看看并理解默认配置文件中的那些属性。虽然默认配置文件能让 Hadoop 核心程序顺利启动，但对于开发人员来说一般需要自己来设置一些常规配置以满足开发和业务的需求，所以我们需要对默认配置文件的值进行覆盖，具体方法如下：

h3. Step2-2-1 core-default.xml 配置

$HADOOP_HOME/conf/core-site.xml 是 Hadoop 的核心配置文件，对应并覆盖 core-default.xml 中的配置项。我们一般在这个文件中增加如下配置：

{% highlight bash linenos %}
<configuration>  
	<property>   
		<!-- 用于dfs命令模块中指定默认的文件系统协议 -->  
		<name>fs.default.name</name>   
		<value>hdfs://192.168.1.111:9000</value> 
</configuration> 
{% endhighlight %} 

h3. Step2-2-2 hdfs-default.xml 配置

$HADOOP_HOME/conf/hdfs-site.xml 是 HDFS 的配置文件，对应并覆盖 hdfs-default.xml 中的配置项。我们一般在这个文件中增加如下配置：

{% highlight bash linenos %}
<configuration>
	<property>
		<name>dfs.replication</name>
		<value>2</value>
	</property>
	
	<property>
		<name>hadoop.tmp.dir</name>
		<value>/home/hadoop/cluster/data</value>
	</property>

	<property>
		<name>dfs.data.dir</name>
		<value>${hadoop.tmp.dir}/dfs/data</value>
	</property> 
</configuration>  
{% endhighlight %} 

h3. Step2-2-3 mapred-default.xml 配置

$HADOOP_HOME/conf/mapred-site.xml 是 Map/Reduce 的配置文件，对应并覆盖 mapred-default.xml 中的配置项。我们一般在这个文件中增加如下配置：

{% highlight bash linenos %}
<configuration>
	<property>
		<name>mapred.job.tracker</name>
		<value>192.168.1.111:9001</value>
		</property>
	</property>
</configuration>
{% endhighlight %}

h2. Step2-3 主从配置

在 $HADOOP_HOME/conf 目录中存在 masters 和 slaves 这两个文件，用来做 Hadoop 的主从配置。上面已经提到了 Hadoop 主要由 NameNode/DataNode 和 JobTracker/TaskTracker 构成，在主从配置里我们一般将 NameNode 和 JobTracker 列为主机，其它的共为从机，于是对于此处的配置应该是：

h3. Step2-3-1 masters 配置

在 masters 中加入：

{% highlight bash linenos %}
192.168.1.111
{% endhighlight %}

h3. Step2-3-2 slaves 配置

在 slaves 中加入：

{% highlight bash linenos %}
192.168.1.112
192.168.1.113
{% endhighlight %}

如果你对以上介绍的配置项做了正确的配置，那么你的 Hadoop 集群只差启动了，当然，在 $HADOOP_HOME/conf 目录下还包括其它的一些配置文件，但那些都不是必须设置的，如果有兴趣你可以自己去了解了解。

h3. 值得注意的是 Hadoop 集群的所有机器的配置应该保持一致，一般我们在配置完 master 后，使用 scp 将配置文件同步到集群的其它服务器上。

