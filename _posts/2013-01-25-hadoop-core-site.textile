---
layout: post
title: Hadoop-1.x core-site.xml 参数设定
category: hadoop
---

h2. 平台环境

* CentOS 6.X
* JDK 1.7
* Hadoop 1.0.4 

h2. 参照 <a href="http://hadoop.apache.org/docs/r1.0.4/core-default.html" target="_blank">core-default.xml</a>

h2. fs.default.name

*预设值* : file:///
*说明* : 设定 Hadoop namenode 的 hostname 及 port，预设是 Standalone mode，如果是 Pseudo-Distributed mode 要指定为 hdfs://localhost:9000，但是这个缺点是只有在本机才能操作，从其他机器不能连。建议可直接使用 Cluster mode，指定 hdfs://hostname:9000。但是 Hadoop 是依据 hostname 去做 ip binding，所以要注意 /etc/hosts 里 hostname 不能对应到 127.0.0.1，要对应实际的 ip。

h2. hadoop.tmp.dir

*预设值* : /tmp/hadoop-${user.name}
*说明* : Hadoop 存放暂存档案的目录，会根据 user account 在此目录下开不同的子目录。但是放在预设的 /tmp 下可能会有一个问题，一般在 Centos 会 enable tmpwatch，tmpwatch 会定期把 /tmp 下沒用到的档案砍掉，如果不希望系统做这件事，可以 disable tmpwatch 或把 hadoop.tmp.dir 指到不同的目录下。

h2. fs.checkpoint.dir

*预设值* : ${hadoop.tmp.dir}/dfs/namesecondary
*说明* : secondary namenode 存放暂存档案的目录，如果有多个目录可用“，”隔开。设定多个目录的好处是 Hadoop 会把 temp image files 分别写到指定的多个目录，以避免其中一份资料坏掉。seconary namenode 相关的设定不一定需要，甚至在 Hadoop cluster 可以不需要起 secondary namenode。但重起 namenode 时也会做 file merge，当档案很大时，重起的时间会非常的长。为了减少 downtime，建议在 production site 都会启动 secondary namenode。而且要起在跟 namenode 不同的机器，以保证当 namenode 硬碟坏掉的時候，还可以从 secondary namenode 上把资料备份回来。

h2. fs.checkpoint.period

*预设值* : 3600(秒)
*说明* : 控制 secondary namenode 的 checkpoint 时间间隔。如果距离上次 checkpoint 的时间大于這个参数设定的值，就会触发 checkpoint。secondary namenode 会把 namenode 的 fsimage 和 editlog 做 snapshot。如果存取 Hadoop 的次数频繁或为了减少重起 namenode 的 downtime，可以把这个值设小一点。

h2. fs.checkpoint.size

*预设值* : 67108864(byte)
*说明* : 如果 Hadoop 非常的忙碌，editlog 可能会在短时间內长的很大，fs.checkpoint.period 的设定不见得可以完全预测这个状况，所以保险的做法会多设定这个值，以保证当档案大到超过 fs.checkpoint.size 的值也会触发 checkpoint。

h2. io.file.buffer.size

*预设值* : 4096(byte)
*说明* : 这是读写 sequence file 的 buffer size, 可减少 I/O 次数。在大型的 Hadoop cluster，建议可设定为 65536 到 131072。

h2. ipc.client.connection.maxidletime

*预设值* : 10000(毫秒)
*说明* : 设定 Hadoop client 连线时最大的闲置时间，预设是 10 秒。如果 Hadoop cluster 的网络连线不稳，可以把這個值設到 60000(60秒)。

h2. ipc.server.tcpnodelay

*预设值* : false
*说明* : 在 Hadoop server 是否启动 Nagle’s 演算法。设 true 会 disable 这个演算法，关掉会减少延迟，但是会增加小封包的传输。server site 不太需要设定这个值。

h2. ipc.client.tcpnodelay

*预设值* : false
*说明* : 在 Hadoop client 是否启动 Nagle’s 演算法。设 true 会 disable 这个演算法，关掉会减少延迟，但是会增加小封包的传输。client site 建议把这个值设 true。

h2. hadoop.security.authorization

*预设值* : false
*说明* : 是不是要开启 service-level 帐号验证机制，开启之后 Hadoop 在执行任何动作之前都会先确认是否有权限。详细的权限设定会放在 hadoop-policy.xml 里。例如要让 fenriswolf 这个 account 及 mapreduce group 可以 submit M/R jobs，要设定 security.job.submission.protocol.acl。

{% highlight bash linenos %}
<property>
	<name>security.job.submission.protocol.acl</name>
	<value>fenriswolf mapreduce</value>
</property>
{% endhighlight %}

h2. hadoop.security.authentication

*预设值* : simple
*说明* : simple 表示沒有 authentication，Hadoop 会用 system account 及 group 来控管权限。另外可以指定为 kerberos，这部分相对比较复杂，要有一个 kerberos server 并产生 account keytab，在执行任何操作前 client 要先用 kinit 指令对 kerberos server 认证，之后的任何操作都是以 kerberos account 来执行。

h2. hadoop.kerberos.kinit.command

*预设值* : N/A
*说明* : 如果 hadoop.security.authentication 设为 kerberos 就要多设这个参数指定 Kerberos kinit 指令的路径。在 CentOS 装 krb5-workstation package 后预设安装路径为 /usr/kerberos/bin/kinit。

h2. fs.trash.interval

*预设值* : 0(分)
*说明* : 清掉垃圾筒的时间。预设是不清, 所以在刪除资料时要自己执行

hadoop fs -rm -skipTrash 或 hadoop fs -expunge 

来清除垃圾筒的资料，但是强制用 -skipTrash 会造成误刪的资料救不回来，user 也常常会忘记做 -expunge 而造成 Hadoop 空间不会释放。建议可以设为 1440 让 Hadoop 每天清除垃圾筒。

h2. topology.script.file.name

*预设值* : N/A
*说明* : 实现 Hadoop Rack Awareness 的机制，指定一个可执行档，input 会是一组 hostname 或 ip，回传值是 rack name 清单。不指定的情況下，Hadoop 会预设所有的 node 都在同一个 rack 之下。
以下是一个 python 的范例，不过用 shell script 或其他语言写也可以

{% highlight bash python %}
#!/usr/bin/python

import sys
from string import join

DEFAULT = '/dc/rack0';

RACK_MAP = {
    '10.1.113.37' : '/dc/rack1',
    'hadoop-worker01' : '/dc/rack1',
    
    '10.1.113.77' : '/dc/rack1',
    'hadoop-worker02' : '/dc/rack1',
    
    '10.1.113.45' : '/dc/rack2',
    'hadoop-work03' : '/dc/rack2',
    
    '10.1.113.48' : '/dc/rack2',
    'hadoop-work04' : '/dc/rack2',
  }

if len(sys.argv) == 1:
    print DEFAULT
else:
    print join([RACK_MAP.get(i, DEFAULT) for i in sys.argv[1:]]," ")
{% endhighlight %}

一个非常大的 Hadoop cluster 可能会跨多个 data centers，每个 data center 会有多个 racks，每个 rack 有多个 nodes。假设 Hadoop replication number 设 3，在 Hadoop 做 replication 时会根据这个设定，第一份资料放在 local node，第二份资料放在另一个 rack 的某个 node，第三份资料会放在与第二份同个 rack 但不同的 node 下。当网络设定有问题或断线时，某一个 rack 可能会全部不见，放在不同的 rack 可以保证仍然能存取到资料。为了增加网络的容错能力，一般都会设定这个 script。
如果在 cluster 已经有资料的情况下才设定 rack topology，可以用 hadoop balancer 指令让所有的 blocks 重新分配

h2. topology.script.number.args

*预设值* : 100
*说明* : 每次传給 topology.script.file.name script 的参数个数。如果 Hadoop node 个数过多，topology.script.file.name script 会被执行多次，一次传入 100 个参数
 

h2. hadoop.native.lib

*预设值* : true
*说明* : 预设 Hadoop 会去找所有可用的 native libraries 并自动 load 进来使用，例如压缩类的 libraries 像 GZIP, LZO 等等。会设成 false 的原因通常是为了 debug，Hadoop 会把 native libraries 换成相对应的 java 实现方式来执行，例如 GZIP，以方便使用者检测 libraries 是否执行错误。但是 LZO 这类的 libraries 并没有 java 实现，所以还是会 call native libraries 来做压缩，也就沒有 debug 的效果了。详细的压缩格式类型会在 mapred-site.xml 的设定时再介绍。


<a href="http://fenriswolf.me/2012/04/05/hadoop-%E5%8F%83%E6%95%B8%E8%A8%AD%E5%AE%9A-core-site-xml/" target="_blank"> >>Hadoop 參數設定 – core-site.xml</a>