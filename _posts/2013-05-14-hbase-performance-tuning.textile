---
layout: post
title: HBase 性能调优
category: hbase
---

h2. 参照 <a href="http://hbase.apache.org/book.html#performance" target="_blank">Apache HBase (TM) Performance Tuning</a>

h2. zookeeper.session.timeout

*预设值* : 3分钟（180000ms）
*说明* : RegionServer 与 Zookeeper 间的连接超时时间。当超时时间到后，ReigonServer 会被 Zookeeper 从 RS 集群清单中移除，HMaster 收到移除通知后，会对这台 server 负责的 regions 重新 balance，让其他存活的 RegionServer 接管。
*调优* : 这个 timeout 决定了 RegionServer 是否能够及时的 failover。
* 设置成1分钟或更低，可以减少因等待超时而被延长的 failover 时间。

* 不过需要注意的是，对于一些 Online 应用，RegionServer 从宕机到恢复时间本身就很短的（网络闪断，crash 等故障，运维可快速介入），如果调低 timeout 时间，反而会得不偿失。因为当 ReigonServer 被正式从 RS 集群中移除时，HMaster 就开始做 balance 了（让其他 RS 根据故障机器记录的 WAL 日志进行恢复）。当故障的 RS 在人工介入恢复后，这个 balance 动作是毫无意义的，反而会使负载不均匀，给 RS 带来更多负担。特别是那些固定分配 regions 的场景。

* 如果集群正在集中处理一些大数据，为避免响应时间过长（假死）而被 Zookeeper 从 RS 集群中移除，应设置成更高的时间。

h2. hbase.regionserver.handler.count

*预设值*: 10
*说明*: RegionServer 的请求处理 IO 线程数。
*调优*: 这个参数的调优与内存息息相关。
* 较少的 IO 线程，适用于处理单次请求内存消耗较高的 Big PUT 场景（大容量单次 PUT 或设置了较大 cache 的 scan，均属于 Big PUT）或 ReigonServer 的内存比较紧张的场景。

* 较多的 IO 线程，适用于单次请求内存消耗低，TPS 要求非常高的场景。设置该值的时候，以监控内存为主要参考。

* 这里需要注意的是如果 server 的 region 数量很少，大量的请求都落在一个region上，因快速充满 memstore 触发 flush 导致的读写锁会影响全局 TPS，不是 IO 线程数越高越好。

* 压测时，开启<a href="http://hbase.apache.org/book.html#rpc.logging" target="_blank">Enabling RPC-level logging</a>，可以同时监控每次请求的内存消耗和 GC 的状况，最后通过多次压测结果来合理调节 IO 线程数。

* 这里是一个案例 <a href="http://software.intel.com/en-us/articles/hadoop-and-hbase-optimization-for-read-intensive-search-applications/" target="_blank">Hadoop and HBase Optimization for Read Intensive Search Applications</a>，作者在 SSD 的机器上设置 IO 线程数为 100，仅供参考。

h2. hbase.hregion.max.filesize

*预设值*: 256M
*说明*: 在当前 ReigonServer 上单个 Reigon 的最大存储空间，单个 Region 超过该值时，这个 Region 会被自动 split 成更小的 region。
*调优*: 
* 小 region 对 split 和 compaction 友好，因为拆分 region 或 compact 小 region 里的 storefile 速度很快，内存占用低。缺点是 split 和 compaction 会很频繁。
特别是数量较多的小 region 不停地 split, compaction，会导致集群响应时间波动很大，region 数量太多不仅给管理上带来麻烦，甚至会引发一些 Hbase 的 bug。
一般512以下的都算小 region!!

* 大 region，则不太适合经常 split 和 compaction，因为做一次 compact和 split 会产生较长时间的停顿，对应用的读写性能冲击非常大。此外，大 region 意味着较大的 storefile，compaction 时对内存也是一个挑战。
当然，大 region 也有其用武之地。如果你的应用场景中，某个时间点的访问量较低，那么在此时做 compact 和 split，既能顺利完成 split 和 compaction，又能保证绝大多数时间平稳的读写性能。

* 既然 split 和 compaction 如此影响性能，有没有办法去掉？
<h3>compaction 是无法避免的，split 倒是可以从自动调整为手动。</h3>
只要通过将这个参数值调大到某个很难达到的值，比如 100G，就可以间接禁用自动 split（RegionServer 不会对未到达 100G 的 region 做 split）。
再配合 <a href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/util/RegionSplitter.html" target="_blank">RegionSplitter</a> 这个工具，在需要 split 时，手动 split。
手动 split 在灵活性和稳定性上比起自动split要高很多，相反，管理成本增加不多，比较推荐 online 实时系统使用。

* 内存方面，小 region 在设置 memstore 的大小值上比较灵活，大 region 则过大过小都不行，过大会导致 flush 时 app 的 IO wait 增高，过小则因 store file 过多影响读性能。

h2. hbase.regionserver.global.memstore.upperLimit/lowerLimit

*预设值*: 
*说明*: 
*调优*: 

{% highlight bash linenos %}
{% endhighlight %}